{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row,Column\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_path = 'station.csv'\n",
    "status_path ='status.csv'\n",
    "trip_path = 'trip.csv'\n",
    "weather_path = 'weather.csv'\n",
    "\n",
    "station_raw = sc.textFile(station_path)\n",
    "\n",
    "station_with_header = station_raw.mapPartitions(lambda x:csv.reader(x))\n",
    "station_header = station_with_header.first()\n",
    "station = station_with_header.filter(lambda x:x!=station_header)\n",
    "\n",
    "status_raw = sc.textFile(status_path)\n",
    "status_with_header = status_raw.mapPartitions(lambda x:csv.reader(x))\n",
    "status_header = status_with_header.first()\n",
    "status = status_with_header.filter(lambda x:x!=status_header)\n",
    "\n",
    "trip_raw = sc.textFile(trip_path)\n",
    "trip_with_header = trip_raw.mapPartitions(lambda x:csv.reader(x))\n",
    "trip_header = trip_with_header.first()\n",
    "trip = trip_with_header.filter(lambda x:x!=trip_header)\n",
    "\n",
    "weather_raw = sc.textFile(weather_path)\n",
    "weather_with_header = weather_raw.mapPartitions(lambda x:csv.reader(x))\n",
    "weather_header = weather_with_header.first()\n",
    "weather = weather_with_header.filter(lambda x:x!=weather_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toIntSafe(inval):\n",
    "  try:\n",
    "    return int(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toFloatSafe(inval):\n",
    "  try:\n",
    "    return float(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toTimeSafe(inval):\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toLong(inval):\n",
    "  try:\n",
    "    return long(float(inval))\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "\n",
    "def toStringSafe(inval):\n",
    "  try:\n",
    "    return str(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toTime_Safe(inval):\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%m/%d/%Y\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def status_to_time(inval):\n",
    "    #'2013/08/29 12:06:01'\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%Y/%m/%d %H:%M:%S\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def trip_to_time(inval):\n",
    "    #8/29/2013 14:14\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%m/%d/%Y %H:%M\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "#8/29/2013\n",
    "def weather_to_time(inval):\n",
    "    #8/29/2013 14:14\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%m/%d/%Y\")\n",
    "  except ValueError:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "# get weekdays and daily hours from timestamp\n",
    "def toWeekDay(x):\n",
    "#     v = datetime.strptime(datetime.fromtimestamp(int(x)).strftime(\"%Y %m %d %H\"), \"%Y %m %d %H\").strftime('%w') - from unix timestamp\n",
    "    v = x.strftime('%w')\n",
    "    return v\n",
    "\n",
    "to_week_day = udf(toWeekDay, StringType())\n",
    "\n",
    "\n",
    "# newdf = elevDF.select(year(elevDF.date).alias('dt_year'), month(elevDF.date).alias('dt_month'), \n",
    "# dayofmonth(elevDF.date).alias('dt_day'), dayofyear(elevDF.date).alias('dt_dayofy'), hour(elevDF.date).alias('dt_hour'), minute(elevDF.date).alias('dt_min'), weekofyear(elevDF.date).alias('dt_week_no'), unix_timestamp(elevDF.date).alias('dt_int'))\n",
    "\n",
    "trip_new = trip_df.withColumn(\"year\",year(trip_df['date']))\\\n",
    "                  .withColumn(\"month\",month(trip_df['date']))\\\n",
    "                  .withColumn(\"day\",dayofmonth(trip_df['date']))\\\n",
    "                  .withColumn(\"dayofweek\", to_week_day(trip_df['date']))\\\n",
    "                  .withColumn(\"hour\",hour(trip_df['date']))\\\n",
    "                  .withColumn(\"minute\",minute(trip_df['date']))\\\n",
    "                  .withColumn(\"second\",second(trip_df['date']))\\\n",
    "                  .drop('date')\\\n",
    "                  .withColumn(\"end_year\",year(trip_df['end_date']))\\\n",
    "                  .withColumn(\"end_month\",month(trip_df['end_date']))\\\n",
    "                  .withColumn(\"end_day\",dayofmonth(trip_df['end_date']))\\\n",
    "                  .withColumn(\"end_hour\",hour(trip_df['end_date']))\\\n",
    "                  .withColumn(\"end_minute\",minute(trip_df['end_date']))\\\n",
    "                  .withColumn(\"end_second\",second(trip_df['end_date']))\\\n",
    "                  .drop('end_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    for column in df.columns:\n",
    "        if column != 'events' and column != 'max_gust_speed_mph':\n",
    "            if df.filter(df[column].isNull()).count() != 0:\n",
    "                value = (df.groupby(df[column]).count().orderBy('count', ascending = False).first())[0]\n",
    "                df = df.na.fill({column:value})\n",
    "        if column == 'events':\n",
    "            df = df.na.replace(\"\",\"nothing\",[column])\n",
    "        if column == 'max_gust_speed_mph':\n",
    "            df = df.drop(column)\n",
    "    return df\n",
    "\n",
    "weather_new_df = impute(weather_df)\n",
    "weather_new_df = weather_new_df.withColumn(\"year\",year(weather_new_df['date']))\\\n",
    "                  .withColumn(\"month\",month(weather_new_df['date']))\\\n",
    "                  .withColumn(\"day\",dayofmonth(weather_new_df['date']))\\\n",
    "                  .drop('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_zip = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/Users/shen/Desktop/USF/MSAN697DCS/project/sf-bay-area-bike-share/city_zip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_city = weather_new_df.join(city_zip, 'zip_code').drop('zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_delete = trip_new.withColumnRenamed('id','user_id')\n",
    "trip_station = trip_delete.join(station_df, (trip_delete.start_station_name == station_df.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_table_new = all_table.filter(all_table['duration']<=340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_name_string = ['city','events','subscription_type','start_station_name','dayofweek']\n",
    "dfnumeric = indexStringColumns(all_table_new, columns_name_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, columns_name_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_name = ['user_id', 'start_station_id', 'zip_code','bike_id','id','lat','long','end_station_name','name','installation_date','end_station_id',\\\n",
    "            'end_year','end_month','end_day','end_hour','end_minute','end_second','minute','second',\\\n",
    "            'max_temperature_f','min_temperature_f','max_dew_point_f','min_dew_point_f','max_humidity','min_humidity',\\\n",
    "            'max_sea_level_pressure_inches','min_sea_level_pressure_inches','max_visibility_miles','min_visibility_miles',\\\n",
    "            'max_wind_Speed_mph']\n",
    "\n",
    "df_new = dfhot.select([c for c in dfhot.columns if c not in drop_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "response = 'duration'\n",
    "\n",
    "\n",
    "input_columns = [c for c in df_new.columns if c != response]\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_columns)\n",
    "lp = va.transform(df_new).select(\"features\",\"duration\").withColumnRenamed(\"duration\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b = lp.randomSplit([0.8,.2],1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
